#  Data Pipeline Framework
## A framework for building data pipelines.

These are utilities for web/datascraping from capture files.
This is for the first stage of a data pipeline.

These utilities provide methods for 
    - filtering out packets based on rules.
    - extracting relevant data objects, for example in embeded json, or in html

### HAR
Utils for parsing HAR files (HTTP Archive).

- Parser
- Restriction

### DOM
Utilities for parsing dom objects.

- Parser


### Text 
Utilities for parsing text objects


### JSON
Utilities for parsing JSON objects

### Examples

A HAR (HTTP Archive) file can be generated by firefox using the console.
<CTRL-Shift-e>
It stores request/response pairs of dictionaries captured by some browsing
session.

The *HAR parser* in Har/Parser.py is used to load the HAR capture file and apply
some rules  against those to filter out requests/responses

```python
har_parser = Har.Parser("/path/to/harfile.json")
```

A rule, defined Har/Restriction.py is a set of common selectors
```python
restriction = {
    'url_regexp': None,
    'mimetype_regex': None,
    'content_type': None,
    'content_regex': None,
}
```

url\_regexp defines some regular expression to match against the url.
the other keys are not yet implemented. might not be useful.


``` python
Restriction(restriction).match_entry(har_entry)
```
will return true if regexp match `har_entry['request']['url']`.

```python
# Find request/response entries where entry['request']['url'] match
# restriction['url_regexp']
entries = har_parser.find_entries(restriction)
```

You can also ```filter_any(restrictions)``` or ```filter_out(restrictions)```,
where restrictions is a list of restrictions.

use these to select the requests which contain the data you wish to extract.
Once you isolated the interesting requests (probably using firefox's packet
inspector), filter the requests to the data source api endpoints.

Then use the *DOM Parser* to extract the relevant bits of information using
xpath expressions, templates and nested templates (usually forum comment
hierarchy).

To extract data from the Dom, we use xpath expressions, and dictionaries
mapping xpath expressions to keys.

```python

extract_template = {
    "comment_id": "commentrenderer/commentid[1]/text()",
    "video_id": "commentrenderer/publishedtimetext//videoid[1]/text()",
    "text": "commentrenderer/contenttext/runs//text()",
    "author": "commentrenderer/authortext/simpletext/text()",
    "votes": "commentrenderer/votecount/simpletext//text()",
    "likes": "commentrenderer/likecount//text()",
    "replies": "commentrenderer/replycount//text()",
    "time": "commentrenderer/publishedtimetext[1]//text/text()",
}
```

The Dom.Parser() class can be instantiated using an XML text, an lxml.etree
element, or a dictionary. a dictionary will be converted to xml internally.

```python
import json
import Dom
for entry in entries:
    url = entry['request']['url']
    txt = entry['response']['content']['text']

    # If txt is already a DOM text (an HTML probably)
    dom_parser = Dom.Parser(txt)

    # Or, if txt is of a JSON repsonse we load this to a dictionary first.
    d = json.loads(txt)
    dom_parser = Dom.Parser(d)
```
TODO finish this... for now look at tests/integration/
